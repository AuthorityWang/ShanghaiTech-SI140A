\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{tikz}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%  

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass : \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%

\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Class
%   - Due date
%   - Name
%   - Student ID

\newcommand{\hmwkTitle}{Homework\ \#014}
\newcommand{\hmwkClass}{Probability \& Statistics for EECS}
\newcommand{\hmwkDueDate}{May 21, 2023}
\newcommand{\hmwkAuthorName}{Penghao Wang}
\newcommand{\hmwkAuthorID}{2021533138}


%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\  \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 23:59}\\
	\vspace{4in}
}

\author{
	Name: \textbf{\hmwkAuthorName} \\
	Student ID: \hmwkAuthorID}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
% Integral dx
\newcommand{\dx}{\mathrm{d}x}
% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}[1]

\begin{enumerate}[(a)]
    \item As for the expected number of occurrences of the expression "CATCAT"
        we have that the probability of "CAT" occurs is that $$p_1p_2p_3, $$ so the 
        probability of "CATCAT" occurs is that $$(p_1p_2p_3)^2, $$ so the expected
        number of occurrences of the expression "CATCAT" is that $$(p_1p_2p_3)^2 \cdot 115 = 115(p_1p_2p_3)^2.$$
    \item Denote that n is the number of C observed in the sequence, 
        then we have that as $p_2$ is a Unif(0, 1), then $$p_2 \sim Beta(1, 1), $$
        then with the Beta-Binomial conjugate prior, we have that $$p_2|n \sim Beta(1 + n, 1 + N - n), $$
        where N is the total observation times, so we have $$p_2 | n \sim Beta(2, 3)$$
        Then as we want to calculate the probability that the next letter is C, we have that the probability
        is that $$E(p_2 | n). $$
        So with the beta distribution, we have that $$E(p_2 | n) = \dfrac{2}{5}. $$
        So given this information, the probability that the next letter is C is $\dfrac{2}{5}$. 
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[2]

\begin{enumerate}[(a)]
    \item According to the method of the bootstrap sample, the $X_j^*$ is generated by 
        sampling with replacement with equal probabilities, so in the $X_j^*$, each $X_j$
        has equal probabilities to be selected. Then we have that as $X_1, ..., X_n$ i.i.d. r.v.s with mean $\mu$ and variance $\sigma^2$
        $$E(X_j^*) = \dfrac{X_1 + X_2 + ... + X_n}{n} = \dfrac{n\mu}{n} = \mu. $$
        $$Var(X_j^*) = \dfrac{Var(X_1 + X_2 + ... + X_n)}{n^2} = \dfrac{n^2Var(X_1)}{n^2} = Var(X_1) = \sigma^2. $$
        Which is the same for all the $X_j^*$, so we have that $$E(X_j^*) = \mu, Var(X_j^*) = \sigma^2 \quad \text{for each j}. $$
    \item As for the $E(\bar{X}^* | X_1, ..., X_n)$ and the $Var(\bar{X}^* | X_1, ..., X_n)$, 
        we have that $$E(\bar{X}^* | X_1, ..., X_n) = \dfrac{1}{n}(E(X_1^*|X_1, ..., X_n) + ... + E(X_n^*|X_2, ..., X_n)) = E(X_1^*|X_1, ..., X_n).$$
        As we have that conditional on $X_1, . . . , X_n$, the $X_j^*$ , $\forall j \in {1, . . . , n}$ are independent,
        so we have that $$E(X_1^*|X_1, ..., X_n) = \bar{X}. $$
        As for the variance, we have that $$Var(\bar{X}^* | X_1, ..., X_n) = \dfrac{n}{n^2}Var(X_1^*|X_1, ..., X_n) = \dfrac{\sum_{j = 1}^{n}(X_j - \bar{X})^2}{n^2}.$$
    \item With Adam's law, we have that 
        $$E(\bar{X}^*) = E(E(\bar{X}^* | X_1, ..., X_n)) = E(\bar{X}) = \mu. $$
        With Eve's law, we have that
        $$\begin{aligned}Var(\bar{X}^*)
            &= E(Var(\bar{X}^* | X_1, ...,X_n)) + Var(E(\bar{X}^* | X_1, ..., X_n)) \\
            &= E(\dfrac{\sum_{j = 1}^{n}(X_j - \bar{X})^2}{n^2}) + Var(\bar{X}) \quad \text{(from question b)}\\
            &= \dfrac{(n - 1)\sigma^2}{n^2} + \dfrac{\sigma^2}{n} \\
            &= \dfrac{2n - 1}{n^2}\sigma^2. \\
        \end{aligned}$$
    \item To intuitively explain $Var(\bar{X}) < Var(\bar{X}^*)$, we have that this is due to the different sampling method, as $\bar{X}^*$ is the 
        that we sample with replacement, which means that each $X_j^*$ may be chosen more than once, 
        while the $\bar{X}$ is just the mean of $X_1, ..., X_n$, so the $X_j^*$ will have more complex situation which will make the variance greater.
        So the variance of $\bar{X}^*$ is larger than that of $\bar{X}$, that is $Var(\bar{X}^*) > Var(\bar{X})$.
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[3]

\begin{enumerate}[(a)]
    \item By solving the problems using conditional expectation. With LOTE and first step analysis, we denote that 
        $O_1$ is the outcome of the first toss, $O_2$ is the outcome of the second toss. The probability of a toss head
        is that $p$, and the probability of a toss tail is that $q$, where $p + q = 1$. 
        \begin{enumerate}[1.]
            \item HT \\
                $$E(W_{HT}) = E(W_{HT} | O_1 = H)P(O_1 = H) + E(W_{HT} | O_1 = T)P(O_1 = T) \quad \text{with LOTE}.$$
                Then as for the $E(W_{HT} | O_1 = H)$, we have that 
                $$\begin{aligned}
                    E(W_{HT} | O_1 = H) 
                        &= E(W_{HT} | O_1 = H, O_2 = H)P(O_2 = H | O_1 = H) \\
                            &\quad + E(W_{HT} | O_1 = H, O_2 = T)P(O_2 = T | O_1 = H) \\
                        &= (1 + E(W_{HT} | O_1 = H))p + 2q \quad \text{, as $O_1$ and $O_2$ are independent. }\\
                \end{aligned}$$
                Then we get that $E(W_{HT} | O_1 = H) = \dfrac{p + 2q}{1 - p} = \dfrac{2 - p}{1 - p}$. \\
                Then as for the $E(W_{HT} | O_1 = T)$, we have that 
                $$E(W_{HT} | O_1 = T) = 1 + E(W_{HT}).$$
                So we have that
                $$\begin{aligned}
                    E(W_{HT}) 
                        &= E(W_{HT} | O_1 = H)P(O_1 = H) + E(W_{HT} | O_1 = T)P(O_1 = T) \\
                        &= (\dfrac{2 - p}{1 - p})p + (1 + E(W_{HT}))q \\
                        &= \dfrac{2p - p^2}{1 - p} + q + qE(W_{HT}) \\
                        &= \dfrac{2p - p^2}{1 - p} + 1 - p + (1 - p)E(W_{HT}) \\
                \end{aligned}$$
                So we get that $$E(W_{HT}) = \dfrac{\dfrac{2p - p^2}{1 - p} + 1 - p}{p} = \dfrac{1}{(1 - p)p} = \dfrac{1}{p} + \dfrac{1}{1 - p}. $$
            \item HH \\
                $$E(W_{HH}) = E(W_{HH} | O_1 = H)P(O_1 = H) + E(W_{HH} | O_1 = T)P(O_1 = T) \quad \text{with LOTE}.$$
                Then as for the $E(W_{HH} | O_1 = H)$, we have that 
                $$\begin{aligned}
                    E(W_{HH} | O_1 = H) 
                        &= E(W_{HH} | O_1 = H, O_2 = H)P(O_2 = H | O_1 = H) \\
                            &\quad + E(W_{HH} | O_1 = H, O_2 = T)P(O_2 = T | O_1 = H) \\
                        &= 2p + (2 + E(W_{HH}))q \quad \text{, as $O_1$ and $O_2$ are independent. }\\
                \end{aligned}$$
                Then as for the $E(W_{HH} | O_1 = T)$, we have that 
                $$E(W_{HH} | O_1 = T) = 1 + E(W_{HH}).$$
                So we have that
                $$\begin{aligned}
                    E(W_{HH}) 
                        &= E(W_{HH} | O_1 = H)P(O_1 = H) + E(W_{HH} | O_1 = T)P(O_1 = T) \\
                        &= (2p + (2 + E(W_{HH}))q)p + (1 + E(W_{HH}))q \\
                        &= 2p^2 + 2pq + pqE(W_{HH}) + q + qE(W_{HH}) \\
                \end{aligned}$$
                So we get that $$E(W_{HH}) = \dfrac{2p^2 + 2pq + q}{1 - pq - q} = \dfrac{p + 1}{p^2} = \dfrac{1}{p} + \dfrac{1}{p^2}. $$
        \end{enumerate}
        So we get that $$E(W_{HT}) = \dfrac{1}{p} + \dfrac{1}{1 - p} \quad \text{and that} \quad E(W_{HH}) = \dfrac{1}{p} + \dfrac{1}{p^2}. $$
    \item Suppose that $p$ is unknown, with a Beta(a, b) prior, we have that 
        $$E(W_{HT}) = E(E(W_{HT} | p)) = E(\dfrac{1}{p}) + E(\dfrac{1}{1 - p}). $$
        $$E(W_{HH}) = E(E(W_{HH} | p)) = E(\dfrac{1}{p}) + E(\dfrac{1}{p^2}). $$
        With LOTUS, as for the $E(\dfrac{1}{p})$, we have that 
        $$E(\dfrac{1}{p}) = \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}p^{a - 2}(1 - p)^{b - 1}dp = \dfrac{\Gamma(a + b)\Gamma(a - 1)\Gamma(b)}{\Gamma(a)\Gamma(b)\Gamma(a + b - 1)} = \dfrac{a + b - 1}{a - 1}$$
        $$E(\dfrac{1}{1 - p}) = \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}p^{a - 1}(1 - p)^{b - 2}dp = \dfrac{\Gamma(a + b)\Gamma(a)\Gamma(b - 1)}{\Gamma(a)\Gamma(b)\Gamma(a + b - 1)} = \dfrac{a + b - 1}{b - 1}$$
        $$E(\dfrac{1}{p^2}) = \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}p^{a - 3}(1 - p)^{b - 1}dp = \dfrac{\Gamma(a + b)\Gamma(a - 2)\Gamma(b)}{\Gamma(a)\Gamma(b)\Gamma(a + b - 2)} = \dfrac{(a + b - 1)(a + b - 2)}{(a - 1)(a - 2)}$$
        Then we take them back, we have that $$E(W_{HT}) = \dfrac{a + b - 1}{a - 1} + \dfrac{a + b - 1}{b - 1}$$
        $$E(W_{HH}) = \dfrac{a + b - 1}{a - 1} + \dfrac{(a + b - 1)(a + b - 2)}{(a - 1)(a - 2)}$$
        
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[4]

\begin{enumerate}[(a)]
    \item Denote that N is the number of rolls, $O_1$ is the first roll values. Then we again use conditional expectation and first step analysis, 
        we have that $$E(N) = E(N | O_1 = 1)P(O_1 = 1) + E(N | O_1 != 1)P(O_1 != 1). $$
        Then as for the $E(N | O_1 = 1)$, we have that 
        $$\begin{aligned}E(N | O_1 = 1) 
            &= \sum_{i = 1}^{6}E(N | O_1 = 1, O_2 = i)P(O_2 = i | O_1 = 1) \\
            &= \dfrac{2}{6} + \dfrac{1 + E(N | O_1 = 1)}{6} + \dfrac{4(2 + E(N))}{6}. 
        \end{aligned}$$
        From where we get that $$E(N | O_1 = 1) = \dfrac{2 + 1 + 8 + 4E(N)}{5} = \dfrac{11 + 4E(N)}{5}. $$
        Then as for the $E(N | O_1 != 1)$, we have that $$E(N | O_1 != 1) = 1 + E(N). $$
        So we have that
        $$\begin{aligned}
            E(N) 
                &= E(N | O_1 = 1)P(O_1 = 1) + E(N | O_1 != 1)P(O_1 != 1) \\
                &= \dfrac{11 + 4E(N)}{5}\dfrac{1}{6} + (1 + E(N))\dfrac{5}{6} \\
                &= \dfrac{11}{30} + \dfrac{4}{30}E(N) + \dfrac{5}{6} + \dfrac{5}{6}E(N) \\
                &= \dfrac{36}{30} + \dfrac{29}{30}E(N)
        \end{aligned}$$
        we have that $E(N) = 36. $
    \item As for the consecutive 1's, it is the same with (a), we 
        denote that N is the number of rolls, $O_1$ is the first roll values. Then we again use conditional expectation and first step analysis, 
        we have that $$E(N) = E(N | O_1 = 1)P(O_1 = 1) + E(N | O_1 != 1)P(O_1 != 1). $$
        Then as for the $E(N | O_1 = 1)$, we have that 
        $$\begin{aligned}E(N | O_1 = 1) 
            &= \sum_{i = 1}^{6}E(N | O_1 = 1, O_2 = i)P(O_2 = i | O_1 = 1) \\
            &= \dfrac{2}{6} + \dfrac{5(2 + E(N))}{6} \\
            &= 2 + \dfrac{5E(N)}{6}. 
        \end{aligned}$$
        Then as for the $E(N | O_1 != 1)$, we have that $$E(N | O_1 != 1) = 1 + E(N). $$
        So we have that
        $$\begin{aligned}
            E(N) 
                &= E(N | O_1 = 1)P(O_1 = 1) + E(N | O_1 != 1)P(O_1 != 1) \\
                &= (2 + \dfrac{5E(N)}{6})\dfrac{1}{6} + (1 + E(N))\dfrac{5}{6} \\
                &= \dfrac{7}{6} + \dfrac{35}{36}E(N) \\
        \end{aligned}$$
        we have that $E(N) = 42. $
    \item As for get the same values n times in a row, we have that 
        as $a_n$ is the expected number of rolls to get the same values n times in a row, we have that
        there are mainly 2 cases, that is the the next roll after $a_n$ is the same with the previous one, or not.\\
        If the next roll is still value j, this cases is of probability $\dfrac{1}{6}$, \\
        If the next roll is not value j, the roll need to re-roll, and this cases is of probability $\dfrac{5}{6}$. \\
        We have that $$a_{n + 1} = \dfrac{a_n + 1}{6} + \dfrac{5}{6}(a_n + a_{n + 1}), $$
        which can be simplified to $$a_{n + 1} = 6a_n + 1. $$
    \item As for the formula for $a_n$ for all $n \geq 1$, we could use the recursive formula from (c).
        We have that $$a_1 = 1, \quad a_2 = 1 + 6, \quad a_3 = 1 + 6 + 6^2, .... $$
        So we have that $$a_n = 1 + 6 + 6^2 + ... + 6^{n - 1} = \dfrac{1 - 6^n}{1 - 6} = \dfrac{6^n - 1}{5} .$$
        As for the $a_7$, we have that $$a_7 = \dfrac{6^7 - 1}{5} = 55987. $$
        
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[5]

\begin{enumerate}[(a)]
    \item As $y = ax + b$ is the equation of the best line for predicting Y from X, 
        then as now we want to use Y to predict X, as we have that $x = \dfrac{y - b}{a}$, 
        we then have that an intuitive guess of the slope is $\dfrac{1}{a}$, which is the revert. 
    \item As for a constant c and an r.v. $V$ such that $Y = cX + V$, 
        we firstly set that $$Cov(X, Y - cX) = Cov(X, Y) - cCov(X, X) = \rho - c = 0, $$
        we then have that $c = \rho$, then we define $V = Y - cX$ as we have $Y = cX + V$. 
        Then we have that $(X, V)$ is Bivariate Normal, so we get that $X$ and $V$ are independent. 
        Then we get that the constant is that $$c = \rho. $$ and the r.v. $V$ is that $$V = Y - \rho X. $$
    \item As for a constant d and an r.v. $W$ such that $X = dY + W$, 
        we firstly set that $$Cov(X - dY, Y) = Cov(X, Y) - dCov(Y, Y) = \rho - d = 0, $$
        we then have that $d = \rho$, then we define $W = X - dY$ as we have $X = dY + W$. 
        Then we have that $(Y, W)$ is Bivariate Normal, so we get that $Y$ and $W$ are independent. 
        Then we get that the constant is that $$d = \rho. $$ and the r.v. $W$ is that $$W = X - \rho Y. $$
    \item As for the $E(Y | X)$ and $E(X | Y)$, we have that 
        as $V = Y - \rho X$ and $W = X - \rho Y$, we have that
        $$E(Y | X) = E(\rho X + V | X) = \rho E(X | X) + E(V | X) = \rho X + E(V) = \rho X. $$
        $$E(X | Y) = E(\rho Y + W | Y) = \rho E(Y | Y) + E(W | Y) = \rho Y + E(W) = \rho Y. $$
    \item Due to the symmetry of the correlation coefficient, the slope of the best linear equation 
        for predicting X from Y is the same as the slope for predicting Y from X, not the reciprocal. 
        This is due to the phenomenon of regression toward the mean, which involves a trade-off between the father and son. 
        If the correlation coefficient is close to 1, we give more weight to the father's height; 
        if the correlation coefficient is close to 0, we give more weight to the population mean. 
        This is because height is not entirely hereditary, and by considering regression toward the mean, 
        we obtain a more reasonable prediction. Therefore, the best guess for X given Y is $E(X) = 0. $
\end{enumerate}

\end{homeworkProblem}

\end{document}
