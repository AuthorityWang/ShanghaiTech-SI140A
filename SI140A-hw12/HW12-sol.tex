\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{tikz}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%  

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass : \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
		\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}


\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%

\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Class
%   - Due date
%   - Name
%   - Student ID

\newcommand{\hmwkTitle}{Homework\ \#12}
\newcommand{\hmwkClass}{Probability \& Statistics for EECS}
\newcommand{\hmwkDueDate}{May 7, 2023}
\newcommand{\hmwkAuthorName}{Wang Penghao}
\newcommand{\hmwkAuthorID}{2021533138}


%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\\  \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 23:59}\\
	\vspace{4in}
}

\author{
	Name: \textbf{\hmwkAuthorName} \\
	Student ID: \hmwkAuthorID}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}
% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
% Integral dx
\newcommand{\dx}{\mathrm{d}x}
% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}
% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}[1]

\begin{enumerate}[(a)]
    \item As for the posterior distribution, we have that is $$p | X_1 = x_1, X_2 = x_2, ..., X_n = x_n$$
        As the prior is Beta(1, 1), then we have that by using the Beta Binomial conjugacy, we get that the posterior
        with $X_1 = x_1$ is that $$Beta(1 + x_1, 1 + (1 - x_1))$$
        The same, with additional given $X_2 = x_2$, we get that the posterior is that
        $$Beta(1 + x_1 + x_2, 1 + (1 - x_1) + (1 - x_2))$$
        So, given that $X_1 = x_1, X_2 = x_2, ..., X_n = x_n$
        we get that the posterior is that $$Beta(1 + \sum_{j = 1}^{n}x_j, 1 + n - \sum_{j = 1}^{n}x_j)$$
        So we get that $$p | X_1 = x_1, X_2 = x_2, ..., X_n = x_n \sim Beta(1 + \sum_{j = 1}^{n}x_j, 1 + n - \sum_{j = 1}^{n}x_j)$$
        From where we see that the parameters of the distribution is only consisted of $\sum_{j = 1}^{n}x_j$
        so we get that the distribution only depends on the sum of $x_j$. 
    \item Proof of Laplace's law of succession, denote the sum of $X_j$ is that
        $$S_n = \sum_{j = 1}^{n}X_j, $$ use the LOTP, denote that $f(p | S_n = k)$ is the posterior PDF, we get that 
        $$P(X_{n + 1} = 1 | S_n = k) = \int_{0}^{1}P(X_{n + 1} = 1 | p, S_n = k)f(p | S_n = k)dp$$
        then we have that 
        $$\begin{aligned}
            P(X_{n+1} = 1 | S_n = k) 
                &= \dfrac{\Gamma(n + 2)}{\Gamma(k + 1)\Gamma(n - k + 1)}\int_{0}^{1}pp^k(1 - p)^{n - k}dp \\
                &= \dfrac{\Gamma(n + 2)}{\Gamma(k + 1)\Gamma(n - k + 1)} \dfrac{\Gamma(k + 2)\Gamma(n - k + 1)}{\Gamma(n + 3)} \\
                &= \dfrac{k + 1}{n + 2}.
        \end{aligned}$$
    \item From the perspective of Beta-Binomial conjugacy, we have that as the 
        Laplace's Law of Succession states that if we observe an event N times, of which k times are successful, 
        then the probability of success in the next event is $\dfrac{k + 1}{N + 2}$. This can be presented as Beta distribution.
        If we assume a prior probability of beta(a,b), then after observing k successes and (N-k) failures, 
        the posterior probability is beta(a+k,b+(N-k)). The Beta distribution has conjugacy with the binomial distribution, 
        which means that we can use the Beta distribution to represent the posterior distribution of the binomial distribution. 
        Therefore, Laplace's Law of Succession can be reinterpreted as a Bayesian inference process on the binomial distribution 
        using the Beta-Binomial conjugacy relationship.
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[2]

\begin{enumerate}[(a)]
    \item By using LOTUS, as we have that $p \sim Beta(a, b)$
        we have that $E(p^2(1 - p)^2)$ can be written as
        $$\begin{aligned}
            E(p^2(1 - p)^2) 
                &= \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}p^2(1 - p)^2p^{a - 1}(1 - p)^{b - 1}dp \\
                &= \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\int_{0}^{1}p^{a + 1}(1 - p)^{b + 1}dp \\
                &= \dfrac{\Gamma(a + b)}{\Gamma(a)\Gamma(b)}\dfrac{\Gamma(a + 2)\Gamma(b + 2)}{\Gamma(a + b + 4)}
        \end{aligned}$$
        With the property of Gamma function, we have that
        $$\Gamma(a + 2) = (a + 1)\Gamma(a + 1) = (a + 1)a\Gamma(a), $$
        and that 
        $$\Gamma(a + b + 4) = (a + b + 3)\Gamma(a + b + 3) = (a + b + 3)(a + b + 2)(a + b + 1)(a + b)\Gamma(a + b). $$
        So we get that 
        $$E(p^2(1 - p)^2) = \dfrac{(a + 1)a(b + 1)b}{(a + b + 3)(a + b + 2)(a + b + 1)(a + b)}$$
    \item The posterior distribution only depends on the fact that A won exactly 6 of the 10 games on record.
        As for a sequence event, we need to update the distribution, firstly, we get Beta(a, b), where a = 1, b = 1, 
        then if a A wins occurs, the a += 1, if a B wins occurs, the b += 1. From the update process, 
        we get that the order of outcomes do not effect the final parameters of Beta distribution, 
        so we get that it is the fact that A won exactly 6 of 10 games on record. 
    \item As the input event is that AAABBAABAB, where A wins 6 times, and B wins 4 times. 
        So we get that the final posterior distribution for p given historical data is 
        $$p | historical data \sim Beta(6 + 1, 4 + 1)$$ which is 
        $$p | historical data \sim Beta(7, 5)$$
    \item As for the indicator of A, we have that they are conditionally independent given p, 
        however, without given p, they are not independent. Thus, given p, the indicators are uncorrelated, 
        without given p, the indicators are postively correlated as if A wins at the first match, 
        the p will increased, as the A is more believed to win. 
    \item As for the probability that the match is not yet decided when going into the fifth game
        There must be 2 wins for A and 2 wins for B, as in (c), we get the posterior distribution is Beta(7, 5), so we get that the probability is
        $$\begin{pmatrix}
            4 \\ 2
        \end{pmatrix} p^2(1 - p)^2 = 6\dfrac{(7 + 1)7(5 + 1)5}{(7 + 5 + 3)(7 + 5 + 2)(7 + 5 + 1)(7 + 5)} = \dfrac{4}{13}. $$
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[3]

\begin{enumerate}
    \item As for the joint PDF of $U_{(1)}, U_{(2)}, ..., U_{(n)}$, 
        as we have that they have the order relationship, we then get that 
        $$0 < u_{(1)} < ... < u_{(n)} < 1, $$
        denote that the joint PDF is $f_{U_{(1)}, ..., U_{(n)}}, $
        we then get that as there are $n!$ possible orderings of the $U_{(i)}$,
        $$f_{U_{(1)}, ..., U_{(n)}} = n!\prod_{i = 1}^{n}f_{U_{(i)}}(u_{(i)})$$
        Then as all the $U_{(i)}$ are i.i.d. uniform distribution, we get that
        $$f_{U_{(1)}, ..., U_{(n)}} = n!\prod_{i = 1}^{n}1 = n!$$
    \item As for the joint PDF of $U_{(j)}, U_{(k)}$, where $1 \leq j < k \leq n$, 
        then we denote that the joint PDF is $f_{U_{(j)}, U_{(k)}}$, we then 
        firstly calculate the number of permutations of $U_{(j)}, U_{(k)}$,
        we firstly need to choose j - 1 from n numbers, then 1 for j, the same for others. 
        We get that the number of permutations is
        $$\begin{pmatrix}
            n \\ 1
        \end{pmatrix}
        \begin{pmatrix}
            n - 1 \\ 1
        \end{pmatrix}
        \begin{pmatrix}
            n - 2 \\ j - 1
        \end{pmatrix}
        \begin{pmatrix}
            n - j - 1 \\ k - j - 1
        \end{pmatrix}
        \begin{pmatrix}
            n - k \\ n - k
        \end{pmatrix}
        = \dfrac{n!}{(j - 1)!(k - j - 1)!(n - k)!}$$
        The same as (a), we get that CDF for Unif distribution for $U_{(k)}$ is that $u_{(k)}$. 
        With the theorem of PDF of order statistic, we then get that the joint PDF is 
        $$\begin{aligned}f_{U_{(j)}, U_{(k)}} 
            &= \dfrac{n!}{(j - 1)!(k - j - 1)!(n - k)!}f(u_{(j)})(u_{(j)})^{j - 1}(u_{(k)} - u_{(j)})^{k - j - 1}f(u_{(k)})(1 - u_{(k)})^{n - k}\\
            &= \dfrac{n!}{(j - 1)!(k - j - 1)!(n - k)!}(u_{(j)})^{j - 1}(u_{(k)} - u_{(j)})^{k - j - 1}(1 - u_{(k)})^{n - k}\end{aligned}$$
    \item As we have that $X \sim Bin(n, p)$ and that $B \sim Beta(j, n - j + 1)$, we then denote that $$U_1, U_2, ..., U_n i.i.d. Unif(0, 1), $$
        which is a sequence of trails, we then denote that $X$ is the number of success, 
        where $X \sim Bin(n, p)$. We then get that $X \geq j$, is the same as the event that $U_{(j)} \leq p$, which is that the j th one in order is at the left of p.
        Then with the theorem of order statistic, we get that
        $$f_{U_{(j)}}(x) = n\begin{pmatrix}
            n - 1 \\ j - 1
        \end{pmatrix}x^{j - 1}(1 - x)^{n - j}$$
        Then we get that the PDF of Beta(j, n - j + 1) distribution is $$f(x) = n\begin{pmatrix}
            n - 1 \\ j - 1
        \end{pmatrix}x^{j - 1}(1 - x)^{n - j}$$which is the same. So we get that $U_{(j)}$ is the same as 
        Beta(j, n - j + 1). So we get that $P(X \geq j) = P(B \leq p)$
    \item As no calculus is allowed, we then have that as 
        for $X \sim Bin(n, p)$ and $B \sim Beta(j, n - j + 1)$, we get that $P(X \geq j) = P(B \leq p)$
        So we get as for the equation, we have that $P(U_{(j)} \leq x)$ is that $$\int_{0}^{x}\dfrac{n!}{(j - 1)!(n - j)!}t_{j - 1}(1 - t)^{n - j}dt$$
        As for the $Bin(n, p)$, the probability of $P(X \geq j)$ is $$P(X \geq j) = \sum_{k = j}^{n}\begin{pmatrix}
            n \\ k
        \end{pmatrix}x^k(1 - x)^{n - k}$$
        As we get in (c), we get that $P(X \geq j) = P(U_{(j)} \leq p)$, so we get that
        $$\int_{0}^{x}\dfrac{n!}{(j - 1)!(n - j)!}t_{j - 1}(1 - t)^{n - j}dt = \sum_{k = j}^{n}\begin{pmatrix}
            n \\ k
        \end{pmatrix}x^k(1 - x)^{n - k}$$
\end{enumerate}

\end{homeworkProblem}

\newpage

\begin{homeworkProblem}[4]
    
As for the Poisson-Gamma Duality, we prove it based on the model of Poisson process. \\
Denote that $X_1, X_2, ..., X_{k + 1}$ i.i.d. Expo(1), then we have that with the property, we get that
$$Z = X_1 + X_2 + ... + X_{k + 1} \sim Gamma(k + 1, 1)$$
Then, we based on the Poisson process model, denote that $N_t$ is the number of arrivals within time t, then we have that $$N_t \sim Pois(\lambda t)$$
which is $$P(N_t = n) = \dfrac{e^{-\lambda t}(\lambda t)^n}{n!}, n \geq 0$$
So we get that $$P(Z > \lambda) = P(N_t \leq k) = \sum_{n = 0}^{k}P(N_\lambda = n) = P(X \leq k)$$
So we based on the Poisson process get that Poisson-Gamma Duality $$P(X \leq k) = P(Z > \lambda)$$


\end{homeworkProblem}

\end{document}